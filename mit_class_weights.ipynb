{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "674a4eb853ef3b8ee2c8318a5f62f960f9c782f62ac23f0329d0e827277c513e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def get_dict_map(data, token_or_tag):\n",
    "    tok2idx = {}\n",
    "    idx2tok = {}\n",
    "    \n",
    "    if token_or_tag == 'token':\n",
    "        vocab = list(set(data['Wort'].to_list()))\n",
    "    else:\n",
    "        vocab = list(set(data['Attribut'].to_list()))\n",
    "    \n",
    "    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n",
    "    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}\n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#from keras.utils import to_categorical\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "def get_pad_train_test_val(data_group, data):\n",
    "    n_token = len(list(set(data['Wort'].to_list())))\n",
    "    n_tag = len(list(set(data['Attribut'].to_list())))\n",
    "\n",
    "    tokens = data_group['Word_idx'].to_list()\n",
    "    maxlen = max([len(s) for s in tokens])\n",
    "    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value=n_token-1)\n",
    "\n",
    "    tags = data_group['Tag_idx'].to_list()\n",
    "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value=tag2idx[\"O\"])\n",
    "\n",
    "    n_tags = len(tag2idx)\n",
    "    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n",
    "\n",
    "    train_tokens, test_tokens, train_tags, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)\n",
    "\n",
    "    print(\n",
    "        'train_tokens length:', len(train_tokens),\n",
    "        '\\ntest_tokens length: ', len(test_tokens),\n",
    "        '\\ntrain_tags length: ', len(train_tags),\n",
    "        '\\ntest_tags length: ', len(test_tags)\n",
    "    )\n",
    "\n",
    "    return train_tokens, test_tokens, train_tags, test_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## class weights berechnen\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "def generate_class_weights(class_series, multi_class=True, one_hot_encoded=False):\n",
    "  \"\"\"\n",
    "  Method to generate class weights given a set of multi-class or multi-label labels, both one-hot-encoded or not.\n",
    "  Some examples of different formats of class_series and their outputs are:\n",
    "    - generate_class_weights(['mango', 'lemon', 'banana', 'mango'], multi_class=True, one_hot_encoded=False)\n",
    "    {'banana': 1.3333333333333333, 'lemon': 1.3333333333333333, 'mango': 0.6666666666666666}\n",
    "    - generate_class_weights([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]], multi_class=True, one_hot_encoded=True)\n",
    "    {0: 0.6666666666666666, 1: 1.3333333333333333, 2: 1.3333333333333333}\n",
    "    - generate_class_weights([['mango', 'lemon'], ['mango'], ['lemon', 'banana'], ['lemon']], multi_class=False, one_hot_encoded=False)\n",
    "    {'banana': 1.3333333333333333, 'lemon': 0.4444444444444444, 'mango': 0.6666666666666666}\n",
    "    - generate_class_weights([[0, 1, 1], [0, 0, 1], [1, 1, 0], [0, 1, 0]], multi_class=False, one_hot_encoded=True)\n",
    "    {0: 1.3333333333333333, 1: 0.4444444444444444, 2: 0.6666666666666666}\n",
    "  The output is a dictionary in the format { class_label: class_weight }. In case the input is one hot encoded, the class_label would be index\n",
    "  of appareance of the label when the dataset was processed. \n",
    "  In multi_class this is np.unique(class_series) and in multi-label np.unique(np.concatenate(class_series)).\n",
    "  Author: Angel Igareta (angel@igareta.com)\n",
    "  \"\"\"\n",
    "  if multi_class:\n",
    "    # If class is one hot encoded, transform to categorical labels to use compute_class_weight   \n",
    "    if one_hot_encoded:\n",
    "      class_series = np.argmax(class_series, axis=1)\n",
    "  \n",
    "    # Compute class weights with sklearn method\n",
    "    class_labels = np.unique(class_series)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=class_series)\n",
    "    return dict(zip(class_labels, class_weights))\n",
    "  else:\n",
    "    # It is neccessary that the multi-label values are one-hot encoded\n",
    "    mlb = None\n",
    "    if not one_hot_encoded:\n",
    "      mlb = MultiLabelBinarizer()\n",
    "      class_series = mlb.fit_transform(class_series)\n",
    "\n",
    "    n_samples = len(class_series)\n",
    "    n_classes = len(class_series[0])\n",
    "\n",
    "    # Count each class frequency\n",
    "    class_count = [0] * n_classes\n",
    "    for classes in class_series:\n",
    "        for index in range(n_classes):\n",
    "            if classes[index] != 0:\n",
    "                class_count[index] += 1\n",
    "    \n",
    "    # Compute class weights using balanced method\n",
    "    class_weights = [n_samples / (n_classes * freq) if freq > 0 else 1 for freq in class_count]\n",
    "    class_labels = range(len(class_weights)) if mlb is None else mlb.classes_\n",
    "    return dict(zip(class_labels, class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "         satzId           Wort       Attribut  Word_idx  Tag_idx\n",
      "0             0          Broan        B-Brand      7419        5\n",
      "1             0  TEN136WWBroan              O     21047        1\n",
      "2             0       TEN136WW  B-Modelnumber     50616        4\n",
      "3             0       Overview              O     51588        1\n",
      "4             0            The              O     32135        1\n",
      "...         ...            ...            ...       ...      ...\n",
      "5441801   16000            any              O     23027        1\n",
      "5441802   16000           size              O     44899        1\n",
      "5441803   16000           hood              O      2191        1\n",
      "5441804   16000              /              O     21107        1\n",
      "5441805   16000     ventilator              O     31326        1\n",
      "\n",
      "[5441806 rows x 5 columns]\n",
      "<ipython-input-4-eed336794442>:14: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  df_group = df.groupby(by = ['satzId'], as_index=False)['Wort', 'Attribut', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))\n",
      "       satzId                                               Wort  \\\n",
      "0           0  [Broan, TEN136WWBroan, TEN136WW, Overview, The...   \n",
      "1           1  [Viking, VCBB5363ERWHViking, VCBB5363ERWH, Ove...   \n",
      "2           2  [Imperial, IS1936PS1TWSB8SSImperial, IS1936PS1...   \n",
      "3           3  [Hestan, KRT485GDNGORHestan, KRT485GDNGOR, Ove...   \n",
      "4           4  [Summit, CP34WMCSummit, CP34WMC, Overview, CP3...   \n",
      "...       ...                                                ...   \n",
      "15996   15996  [Hestan, KRBL36PPHestan, KRBL36PP, Overview, T...   \n",
      "15997   15997  [Smeg, KSM30XUSmeg, KSM30XU, Overview, The, Sm...   \n",
      "15998   15998  [Samsung, NX58T7511SGSamsung, NX58T7511SG, Ove...   \n",
      "15999   15999  [LG, Studio, WSGX201HNALG, Studio, WSGX201HNA,...   \n",
      "16000   16000  [Viking, VWH548481ABViking, VWH548481AB, Overv...   \n",
      "\n",
      "                                                Attribut  \\\n",
      "0      [B-Brand, O, B-Modelnumber, O, O, O, O, O, O, ...   \n",
      "1      [B-Brand, O, B-Modelnumber, O, O, O, O, O, O, ...   \n",
      "2      [B-Brand, O, B-Modelnumber, O, O, O, O, O, O, ...   \n",
      "3      [B-Brand, O, B-Modelnumber, O, O, O, O, O, O, ...   \n",
      "4      [B-Brand, O, B-Modelnumber, O, B-Modelnumber, ...   \n",
      "...                                                  ...   \n",
      "15996  [B-Brand, O, B-Modelnumber, O, O, O, O, O, O, ...   \n",
      "15997  [B-Brand, O, B-Modelnumber, O, O, B-Brand, O, ...   \n",
      "15998  [B-Brand, O, B-Modelnumber, O, O, O, O, O, O, ...   \n",
      "15999  [B-Brand, O, O, O, B-Modelnumber, O, B-Modelnu...   \n",
      "16000  [B-Brand, O, B-Modelnumber, O, O, O, O, O, O, ...   \n",
      "\n",
      "                                                Word_idx  \\\n",
      "0      [7419, 21047, 50616, 51588, 32135, 48577, 2679...   \n",
      "1      [15385, 20624, 42052, 51588, 32135, 24675, 403...   \n",
      "2      [30585, 4977, 33445, 51588, 37939, 45681, 2697...   \n",
      "3      [51491, 41110, 37393, 51588, 32135, 2706, 4034...   \n",
      "4      [35297, 22791, 29838, 51588, 29838, 19041, 403...   \n",
      "...                                                  ...   \n",
      "15996  [51491, 47195, 12887, 51588, 32135, 24675, 403...   \n",
      "15997  [37443, 1997, 28470, 51588, 32135, 37443, 6453...   \n",
      "15998  [30116, 11938, 16948, 51588, 37939, 18781, 142...   \n",
      "15999  [28271, 10050, 13874, 10050, 7191, 51588, 7191...   \n",
      "16000  [15385, 6839, 9529, 51588, 12626, 42427, 4445,...   \n",
      "\n",
      "                                                 Tag_idx  \n",
      "0      [5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, ...  \n",
      "1      [5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "2      [5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, ...  \n",
      "3      [5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, ...  \n",
      "4      [5, 1, 4, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, ...  \n",
      "...                                                  ...  \n",
      "15996  [5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, ...  \n",
      "15997  [5, 1, 4, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "15998  [5, 1, 4, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, ...  \n",
      "15999  [5, 1, 1, 1, 4, 1, 4, 1, 1, 1, 1, 5, 1, 1, 1, ...  \n",
      "16000  [5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "\n",
      "[16001 rows x 5 columns]\n",
      "train_tokens length: 14400 \n",
      "test_tokens length:  1601 \n",
      "train_tags length:  14400 \n",
      "test_tags length:  1601\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('data/trainingdata.csv',escapechar=\"\\\\\",sep=\",\",error_bad_lines=False,warn_bad_lines=False)\n",
    "#print(len(df))\n",
    "\n",
    "token2idx, idx2token = get_dict_map(df, 'token')\n",
    "tag2idx, idx2tag = get_dict_map(df, 'tag')\n",
    "\n",
    "df['Word_idx'] = df['Wort'].map(token2idx)\n",
    "df['Tag_idx'] = df['Attribut'].map(tag2idx)\n",
    "print(df)\n",
    "\n",
    "df_group = df.groupby(by = ['satzId'], as_index=False)['Wort', 'Attribut', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))\n",
    "print(df_group)\n",
    "train_tokens, test_tokens, train_tags, test_tags = get_pad_train_test_val(df_group, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 0. 0. 0. 0. 1.]\n [0. 1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1. 0.]\n ...\n [0. 1. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_dim:  51605 \noutput_dim:  32 \ninput_length:  963 \nn_tags:  6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_dim = len(list(set(df['Wort'].to_list())))+1\n",
    "output_dim = 32\n",
    "input_length = max([len(s) for s in df_group['Word_idx'].tolist()])\n",
    "n_tags = len(tag2idx)\n",
    "print('input_dim: ', input_dim, '\\noutput_dim: ', output_dim, '\\ninput_length: ', input_length, '\\nn_tags: ', n_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "import tensorflow\n",
    "seed(1)\n",
    "tensorflow.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import tensorflow\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "\n",
    "def get_bilstm_lstm_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode='concat'))\n",
    "\n",
    "    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "    model.add(TimeDistributed(Dense(n_tags, activation=\"softmax\")))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'], sample_weight_mode='temporal')\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "array = []\n",
    "\n",
    "for x in range(0, 5000):\n",
    "    for y in range(0, 962):\n",
    "        array.append(train_tags[x][y])\n",
    "\n",
    "#print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: 27643.67816091954, 1: 0.16753142137140872, 2: 633.2280147446024, 3: 438.7885422368181, 4: 154.5828512662296, 5: 48.527037933817596}\n"
     ]
    }
   ],
   "source": [
    "class_weights = generate_class_weights(array, multi_class=True, one_hot_encoded=True)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   IDX            Tag\n0    0  E-Modelnumber\n1    1              O\n2    2        I-Brand\n3    3        E-Brand\n4    4  B-Modelnumber\n5    5        B-Brand\n"
     ]
    }
   ],
   "source": [
    "df_zu_testzwecken = pd.DataFrame(data=[0,1,2,3,4,5], columns=['IDX'])\n",
    "df_zu_testzwecken['Tag'] = df_zu_testzwecken['IDX'].map(idx2tag)\n",
    "print(df_zu_testzwecken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 48.52703793   0.16753142 154.58285127 ...   0.16753142   0.16753142\n    0.16753142]\n [ 48.52703793   0.16753142 154.58285127 ...   0.16753142   0.16753142\n    0.16753142]\n [ 48.52703793   0.16753142 154.58285127 ...   0.16753142   0.16753142\n    0.16753142]\n ...\n [ 48.52703793   0.16753142 154.58285127 ...   0.16753142   0.16753142\n    0.16753142]\n [ 48.52703793   0.16753142 154.58285127 ...   0.16753142   0.16753142\n    0.16753142]\n [  0.           0.           0.         ...   0.           0.\n    0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# ToDo: Array korreckt bef√ºllen:\n",
    "# https://github.com/keras-team/keras/issues/3653#issuecomment-761085597\n",
    "def generate_sample_weights(train_tags, class_weights): \n",
    "    #replaces values for up to 3 classes with the values from class_weights#\n",
    "    # ToDo: Train Tags sind OnHot Encodings \n",
    "        # If class is one hot encoded, transform to categorical labels to use compute_class_weight   \n",
    "\n",
    "    train_tags = np.argmax(train_tags, axis=1)\n",
    "\n",
    "    sample_weights = [np.where(y==0,class_weights[0],\n",
    "                        np.where(y==1,class_weights[1],\n",
    "                        np.where(y==2,class_weights[2],\n",
    "                        np.where(y==3,class_weights[3],\n",
    "                        np.where(y==4,class_weights[4],\n",
    "                        np.where(y==5,class_weights[5],y)))))) for y in train_tags]\n",
    "    return np.asarray(sample_weights)\n",
    "\n",
    "\n",
    "#class_weights = generate_class_weights(array, multi_class=True, one_hot_encoded=True)\n",
    "#print(class_weights)\n",
    "#class_weights = {0: 26722.222222222223, 1: 0.167523264314445, 2: 619.047619047619, 3: 513.8888888888889, 4: 157.03558602677114, 5: 48.278630934457496} \n",
    "# generiert mit 1000 Beispiel Daten\n",
    "class_weights = {0: 27643.67816091954, 1: 0.16753142137140872, 2: 633.2280147446024, 3: 438.7885422368181, 4: 154.5828512662296, 5: 48.527037933817596}\n",
    "# generiert mit 5000 Beispiel Daten\n",
    "# 0 = E-Modelnumber, 1 = O , 2 = I-Brand, 3 = E-Brand, 4 = B-Modelnumber, 5 = B-Brand\n",
    "\n",
    "sample_weights = np.zeros((11520, 963))\n",
    "for x in range(0, 11519):\n",
    "    sample_weights[x] = generate_sample_weights(train_tags[x], class_weights)\n",
    "    \n",
    "print(sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[4.85270379e+01 6.33228015e+02 4.38788542e+02 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.54582851e+02 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 4.85270379e+01 6.33228015e+02\n 4.38788542e+02 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01 1.67531421e-01\n 1.67531421e-01 1.67531421e-01 1.67531421e-01]\n"
     ]
    }
   ],
   "source": [
    "print(sample_weights[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(X, y, model):\n",
    "    loss  = list()\n",
    "    for _ in range(1):\n",
    "        hist = model.fit(X, y, batch_size=256, verbose=1, epochs=1, validation_split=0.2, sample_weight=sample_weights)\n",
    "        loss.append(hist.history['loss'][0])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 963, 32)           1651360   \n_________________________________________________________________\nbidirectional (Bidirectional (None, 963, 64)           16640     \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 963, 32)           12416     \n_________________________________________________________________\ntime_distributed (TimeDistri (None, 963, 6)            198       \n=================================================================\nTotal params: 1,680,614\nTrainable params: 1,680,614\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bilstm_lstm = get_bilstm_lstm_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "45/45 [==============================] - 869s 19s/step - loss: 1.7262 - accuracy: 0.8565 - val_loss: 0.4471 - val_accuracy: 0.8652\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "results['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: models/mein_model_first_try_mit_gewichten_400_fuer_alle_Klassen\\assets\n"
     ]
    }
   ],
   "source": [
    "#model_bilstm_lstm.save('models/mein_model_first_try_mit_gewichten_400_fuer_alle_Klassen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow import keras\n",
    "#reconstructed_model = keras.models.load_model(\"models/mein_model_first_try_mit_gewichten_400_fuer_alle_Klassen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_array = model_bilstm_lstm.predict(test_tokens[56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[0.17599447, 0.15667543, 0.16210331, 0.16892578, 0.17187174,\n",
       "         0.16442932]],\n",
       "\n",
       "       [[0.17531826, 0.1583959 , 0.16269267, 0.1687426 , 0.17056361,\n",
       "         0.16428699]],\n",
       "\n",
       "       [[0.17489554, 0.15950447, 0.16221257, 0.16812703, 0.17050196,\n",
       "         0.16475841]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.17324765, 0.16633794, 0.16230917, 0.16630983, 0.16806856,\n",
       "         0.16372687]],\n",
       "\n",
       "       [[0.17324765, 0.16633794, 0.16230917, 0.16630983, 0.16806856,\n",
       "         0.16372687]],\n",
       "\n",
       "       [[0.17324765, 0.16633794, 0.16230917, 0.16630983, 0.16806856,\n",
       "         0.16372687]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "test_pred_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "predictions = np.argmax(test_pred_array, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "train_tags[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}