{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "674a4eb853ef3b8ee2c8318a5f62f960f9c782f62ac23f0329d0e827277c513e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from itertools import chain\r\n",
    "\r\n",
    "def get_dict_map(data, token_or_tag):\r\n",
    "    tok2idx = {}\r\n",
    "    idx2tok = {}\r\n",
    "    \r\n",
    "    if token_or_tag == 'token':\r\n",
    "        vocab = list(set(data['Wort'].to_list()))\r\n",
    "    else:\r\n",
    "        vocab = list(set(data['Attribut'].to_list()))\r\n",
    "    \r\n",
    "    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\r\n",
    "    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}\r\n",
    "    return tok2idx, idx2tok"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "from keras.preprocessing.sequence import pad_sequences\r\n",
    "#from keras.utils import to_categorical\r\n",
    "from keras.utils.np_utils import to_categorical\r\n",
    "\r\n",
    "def get_pad_train_test_val(data_group, data):\r\n",
    "    n_token = len(list(set(data['Wort'].to_list())))\r\n",
    "    n_tag = len(list(set(data['Attribut'].to_list())))\r\n",
    "\r\n",
    "    tokens = data_group['Word_idx'].to_list()\r\n",
    "    maxlen = max([len(s) for s in tokens])\r\n",
    "    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value=n_token-1)\r\n",
    "\r\n",
    "    tags = data_group['Tag_idx'].to_list()\r\n",
    "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value=tag2idx[\"O\"])\r\n",
    "\r\n",
    "    n_tags = len(tag2idx)\r\n",
    "    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\r\n",
    "\r\n",
    "    train_tokens, test_tokens, train_tags, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)\r\n",
    "\r\n",
    "    print(\r\n",
    "        'train_tokens length:', len(train_tokens),\r\n",
    "        '\\ntest_tokens length: ', len(test_tokens),\r\n",
    "        '\\ntrain_tags length: ', len(train_tags),\r\n",
    "        '\\ntest_tags length: ', len(test_tags)\r\n",
    "    )\r\n",
    "\r\n",
    "    return train_tokens, test_tokens, train_tags, test_tags"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "## class weights berechnen\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "from sklearn.utils.class_weight import compute_class_weight\r\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\r\n",
    "\r\n",
    "\r\n",
    "def generate_class_weights(class_series, multi_class=True, one_hot_encoded=False):\r\n",
    "  \"\"\"\r\n",
    "  Method to generate class weights given a set of multi-class or multi-label labels, both one-hot-encoded or not.\r\n",
    "  Some examples of different formats of class_series and their outputs are:\r\n",
    "    - generate_class_weights(['mango', 'lemon', 'banana', 'mango'], multi_class=True, one_hot_encoded=False)\r\n",
    "    {'banana': 1.3333333333333333, 'lemon': 1.3333333333333333, 'mango': 0.6666666666666666}\r\n",
    "    - generate_class_weights([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]], multi_class=True, one_hot_encoded=True)\r\n",
    "    {0: 0.6666666666666666, 1: 1.3333333333333333, 2: 1.3333333333333333}\r\n",
    "    - generate_class_weights([['mango', 'lemon'], ['mango'], ['lemon', 'banana'], ['lemon']], multi_class=False, one_hot_encoded=False)\r\n",
    "    {'banana': 1.3333333333333333, 'lemon': 0.4444444444444444, 'mango': 0.6666666666666666}\r\n",
    "    - generate_class_weights([[0, 1, 1], [0, 0, 1], [1, 1, 0], [0, 1, 0]], multi_class=False, one_hot_encoded=True)\r\n",
    "    {0: 1.3333333333333333, 1: 0.4444444444444444, 2: 0.6666666666666666}\r\n",
    "  The output is a dictionary in the format { class_label: class_weight }. In case the input is one hot encoded, the class_label would be index\r\n",
    "  of appareance of the label when the dataset was processed. \r\n",
    "  In multi_class this is np.unique(class_series) and in multi-label np.unique(np.concatenate(class_series)).\r\n",
    "  Author: Angel Igareta (angel@igareta.com)\r\n",
    "  \"\"\"\r\n",
    "  if multi_class:\r\n",
    "    # If class is one hot encoded, transform to categorical labels to use compute_class_weight   \r\n",
    "    if one_hot_encoded:\r\n",
    "      class_series = np.argmax(class_series, axis=1)\r\n",
    "  \r\n",
    "    # Compute class weights with sklearn method\r\n",
    "    class_labels = np.unique(class_series)\r\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=class_series)\r\n",
    "    return dict(zip(class_labels, class_weights))\r\n",
    "  else:\r\n",
    "    # It is neccessary that the multi-label values are one-hot encoded\r\n",
    "    mlb = None\r\n",
    "    if not one_hot_encoded:\r\n",
    "      mlb = MultiLabelBinarizer()\r\n",
    "      class_series = mlb.fit_transform(class_series)\r\n",
    "\r\n",
    "    n_samples = len(class_series)\r\n",
    "    n_classes = len(class_series[0])\r\n",
    "\r\n",
    "    # Count each class frequency\r\n",
    "    class_count = [0] * n_classes\r\n",
    "    for classes in class_series:\r\n",
    "        for index in range(n_classes):\r\n",
    "            if classes[index] != 0:\r\n",
    "                class_count[index] += 1\r\n",
    "    \r\n",
    "    # Compute class weights using balanced method\r\n",
    "    class_weights = [n_samples / (n_classes * freq) if freq > 0 else 1 for freq in class_count]\r\n",
    "    class_labels = range(len(class_weights)) if mlb is None else mlb.classes_\r\n",
    "    return dict(zip(class_labels, class_weights))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('data/trainingdata.csv',escapechar=\"\\\\\",sep=\",\",error_bad_lines=False,warn_bad_lines=False)\n",
    "#print(len(df))\n",
    "\n",
    "token2idx, idx2token = get_dict_map(df, 'token')\n",
    "tag2idx, idx2tag = get_dict_map(df, 'tag')\n",
    "\n",
    "df['Word_idx'] = df['Wort'].map(token2idx)\n",
    "df['Tag_idx'] = df['Attribut'].map(tag2idx)\n",
    "print(df)\n",
    "\n",
    "df_group = df.groupby(by = ['satzId'], as_index=False)['Wort', 'Attribut', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))\n",
    "print(df_group)\n",
    "train_tokens, test_tokens, train_tags, test_tags = get_pad_train_test_val(df_group, df)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "         satzId           Wort       Attribut  Word_idx  Tag_idx\n",
      "0             0          Broan        B-Brand     42158        1\n",
      "1             0  TEN136WWBroan              O     80808        7\n",
      "2             0       TEN136WW  B-Modelnumber     31930        3\n",
      "3             0       Overview              O     47429        7\n",
      "4             0            The              O     99583        7\n",
      "...         ...            ...            ...       ...      ...\n",
      "4928479   55339            AND              O     79879        7\n",
      "4928480   55339    MAINTENANCE              O     57644        7\n",
      "4928481   55339       Lockable              O     33649        7\n",
      "4928482   55339        control              O     12113        7\n",
      "4928483   55339          panel              O     45285        7\n",
      "\n",
      "[4928484 rows x 5 columns]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-5-1b4f3e83a784>:14: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  df_group = df.groupby(by = ['satzId'], as_index=False)['Wort', 'Attribut', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "       satzId                                               Wort  \\\n",
      "0           0  [Broan, TEN136WWBroan, TEN136WW, Overview, The...   \n",
      "1           2  [Imperial, IS1936PS1TWSB8SSImperial, IS1936PS1...   \n",
      "2           3  [Hestan, KRT485GDNGORHestan, KRT485GDNGOR, Ove...   \n",
      "3           5  [Hestan, KVP54PPHestan, KVP54PP, Overview, The...   \n",
      "4           6  [LG, LWS3063BDLG, LWS3063BD, Overview, This, 3...   \n",
      "...       ...                                                ...   \n",
      "39006   55334  [Wolf, ®, 36, \", Pro, Style, Gas, Range, -, St...   \n",
      "39007   55336  [LG, 24, \", Smooth, Black, Built, In, Dishwash...   \n",
      "39008   55337  [Whirlpool, ®, 4.3, Cu, ., Ft, ., Chrome, Shad...   \n",
      "39009   55338  [Thermador, ®, Masterpiece, ®, 36, \", Electric...   \n",
      "39010   55339  [ASKO, Classic, 4.1, Cu, ., Ft, ., White, Fron...   \n",
      "\n",
      "                                                Attribut  \\\n",
      "0      [B-Brand, O, B-Modelnumber, O, O, O, O, O, O, ...   \n",
      "1      [B-Brand, O, B-Modelnumber, O, O, O, O, O, O, ...   \n",
      "2      [B-Brand, O, B-Modelnumber, O, O, O, O, O, O, ...   \n",
      "3      [B-Brand, O, B-Modelnumber, O, O, O, O, O, O, ...   \n",
      "4      [B-Brand, O, B-Modelnumber, O, O, O, O, O, O, ...   \n",
      "...                                                  ...   \n",
      "39006  [B-Brand, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
      "39007  [B-Brand, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
      "39008  [B-Brand, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
      "39009  [B-Brand, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
      "39010  [B-Brand, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
      "\n",
      "                                                Word_idx  \\\n",
      "0      [42158, 80808, 31930, 47429, 99583, 23891, 596...   \n",
      "1      [80060, 990, 58205, 47429, 22566, 65557, 72859...   \n",
      "2      [80706, 18061, 100843, 47429, 99583, 68005, 76...   \n",
      "3      [80706, 20962, 47304, 47429, 99583, 98663, 761...   \n",
      "4      [23392, 81042, 12573, 47429, 22566, 18949, 761...   \n",
      "...                                                  ...   \n",
      "39006  [23367, 5260, 8304, 76190, 1661, 53103, 92494,...   \n",
      "39007  [23392, 33030, 76190, 27801, 15864, 91447, 206...   \n",
      "39008  [63481, 5260, 68900, 58008, 56786, 21217, 5678...   \n",
      "39009  [71348, 5260, 63132, 5260, 8304, 76190, 4684, ...   \n",
      "39010  [38234, 97580, 103150, 58008, 56786, 21217, 56...   \n",
      "\n",
      "                                                 Tag_idx  \n",
      "0      [1, 7, 3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, ...  \n",
      "1      [1, 7, 3, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7, 7, 7, ...  \n",
      "2      [1, 7, 3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7, ...  \n",
      "3      [1, 7, 3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, ...  \n",
      "4      [1, 7, 3, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7, 7, ...  \n",
      "...                                                  ...  \n",
      "39006  [1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...  \n",
      "39007  [1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...  \n",
      "39008  [1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...  \n",
      "39009  [1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...  \n",
      "39010  [1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...  \n",
      "\n",
      "[39011 rows x 5 columns]\n",
      "train_tokens length: 35109 \n",
      "test_tokens length:  3902 \n",
      "train_tags length:  35109 \n",
      "test_tags length:  3902\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print(train_tags[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "\r\n",
    "input_dim = len(list(set(df['Wort'].to_list())))+1\r\n",
    "output_dim = 32\r\n",
    "input_length = max([len(s) for s in df_group['Word_idx'].tolist()])\r\n",
    "n_tags = len(tag2idx)\r\n",
    "print('input_dim: ', input_dim, '\\noutput_dim: ', output_dim, '\\ninput_length: ', input_length, '\\nn_tags: ', n_tags)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_dim:  103349 \n",
      "output_dim:  32 \n",
      "input_length:  300 \n",
      "n_tags:  8\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from numpy.random import seed\r\n",
    "import tensorflow\r\n",
    "seed(1)\r\n",
    "tensorflow.random.set_seed(2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import numpy as np\r\n",
    "#import tensorflow\r\n",
    "from tensorflow.keras import Sequential, Model, Input\r\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\r\n",
    "\r\n",
    "def get_bilstm_lstm_model():\r\n",
    "    model = Sequential()\r\n",
    "\r\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\r\n",
    "\r\n",
    "    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode='concat'))\r\n",
    "\r\n",
    "    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\r\n",
    "\r\n",
    "    model.add(TimeDistributed(Dense(n_tags, activation=\"softmax\")))\r\n",
    "\r\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'], sample_weight_mode='temporal')\r\n",
    "    model.summary()\r\n",
    "\r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "array = []\r\n",
    "\r\n",
    "for x in range(0, 5000):\r\n",
    "    for y in range(0, 300):\r\n",
    "        array.append(train_tags[x][y])\r\n",
    "\r\n",
    "#print(array)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class_weights = generate_class_weights(array, multi_class=True, one_hot_encoded=True)\r\n",
    "print(class_weights)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{1: 25.22492222315648, 2: 364.4314868804665, 3: 51.05687736138058, 4: 255.40609569215053, 5: 1382.4884792626729, 6: 249.45950440711792, 7: 0.1443130693090454}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "df_zu_testzwecken = pd.DataFrame(data=[0,1,2,3,4,5,6,7], columns=['IDX'])\r\n",
    "df_zu_testzwecken['Tag'] = df_zu_testzwecken['IDX'].map(idx2tag)\r\n",
    "print(df_zu_testzwecken)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   IDX            Tag\n",
      "0    0            NaN\n",
      "1    1        B-Brand\n",
      "2    2        E-Brand\n",
      "3    3  B-Modelnumber\n",
      "4    4  I-Modelnumber\n",
      "5    5        I-Brand\n",
      "6    6  E-Modelnumber\n",
      "7    7              O\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ToDo: Array korreckt befüllen:\r\n",
    "# https://github.com/keras-team/keras/issues/3653#issuecomment-761085597\r\n",
    "def generate_sample_weights(train_tags, class_weights): \r\n",
    "    #replaces values for up to 3 classes with the values from class_weights#\r\n",
    "    # ToDo: Train Tags sind OnHot Encodings \r\n",
    "        # If class is one hot encoded, transform to categorical labels to use compute_class_weight   \r\n",
    "\r\n",
    "    train_tags = np.argmax(train_tags, axis=1)\r\n",
    "\r\n",
    "    sample_weights = [np.where(y==0,class_weights[0],\r\n",
    "                        np.where(y==1,class_weights[1],\r\n",
    "                        np.where(y==2,class_weights[2],\r\n",
    "                        np.where(y==3,class_weights[3],\r\n",
    "                        np.where(y==4,class_weights[4],\r\n",
    "                        np.where(y==5,class_weights[5],\r\n",
    "                        np.where(y==6,class_weights[6],\r\n",
    "                        np.where(y==7,class_weights[7],\r\n",
    "                        y)))))))) for y in train_tags]\r\n",
    "    return np.asarray(sample_weights)\r\n",
    "\r\n",
    "\r\n",
    "#class_weights = generate_class_weights(array, multi_class=True, one_hot_encoded=True)\r\n",
    "#print(class_weights)\r\n",
    "#class_weights = {0: 26722.222222222223, 1: 0.167523264314445, 2: 619.047619047619, 3: 513.8888888888889, 4: 157.03558602677114, 5: 48.278630934457496} \r\n",
    "# generiert mit 1000 Beispiel Daten\r\n",
    "# class_weights = {0: 27643.67816091954, 1: 0.16753142137140872, 2: 633.2280147446024, 3: 438.7885422368181, 4: 154.5828512662296, 5: 48.527037933817596}\r\n",
    "# generiert mit 5000 Beispiel Daten\r\n",
    "class_weights = {0: 0.00000001, 1: 25.22492222315648, 2: 364.4314868804665, 3: 51.05687736138058, 4: 255.40609569215053, 5: 1382.4884792626729, 6: 249.45950440711792, 7: 0.1443130693090454}\r\n",
    "# 0 = E-Modelnumber, 1 = O , 2 = I-Brand, 3 = E-Brand, 4 = B-Modelnumber, 5 = B-Brand\r\n",
    "\r\n",
    "sample_weights = np.zeros((len(train_tags), 300))\r\n",
    "for x in range(0, len(train_tags)-1):\r\n",
    "    sample_weights[x] = generate_sample_weights(train_tags[x], class_weights)\r\n",
    "    \r\n",
    "# print(sample_weights)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(sample_weights[12])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "def train_model(X, y, model):\r\n",
    "    loss  = list()\r\n",
    "    for _ in range(1):\r\n",
    "        hist = model.fit(X, y, batch_size=256, verbose=1, epochs=1, validation_split=0.2, sample_weight=sample_weights)\r\n",
    "        loss.append(hist.history['loss'][0])\r\n",
    "    return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "model_bilstm_lstm = get_bilstm_lstm_model()\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 32)           3307168   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 300, 64)           16640     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 300, 32)           12416     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 300, 8)            264       \n",
      "=================================================================\n",
      "Total params: 3,336,488\n",
      "Trainable params: 3,336,488\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "results = pd.DataFrame()\r\n",
    "results['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "110/110 [==============================] - 737s 7s/step - loss: 1.6094 - accuracy: 0.8702 - val_loss: 1.1134 - val_accuracy: 0.8400\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "#model_bilstm_lstm.save('models/mein_model_first_try_mit_gewichten_400_fuer_alle_Klassen')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: models/mein_model_first_try_mit_gewichten_400_fuer_alle_Klassen\\assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "#from tensorflow import keras\r\n",
    "#reconstructed_model = keras.models.load_model(\"models/mein_model_first_try_mit_gewichten_400_fuer_alle_Klassen\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "test_pred_array = model_bilstm_lstm.predict(test_tokens[56])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 300) for input KerasTensor(type_spec=TensorSpec(shape=(None, 300), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 1).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "test_pred_array"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[0.10676164, 0.14185148, 0.12226228, ..., 0.10915966,\n",
       "         0.1326139 , 0.12375339]],\n",
       "\n",
       "       [[0.10720688, 0.13631286, 0.1347569 , ..., 0.12932064,\n",
       "         0.12818693, 0.11069626]],\n",
       "\n",
       "       [[0.10733339, 0.13600515, 0.13176312, ..., 0.12506181,\n",
       "         0.13034607, 0.11249956]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.10667396, 0.14034708, 0.13128717, ..., 0.12231664,\n",
       "         0.12716496, 0.12246663]],\n",
       "\n",
       "       [[0.10667396, 0.14034708, 0.13128717, ..., 0.12231664,\n",
       "         0.12716496, 0.12246663]],\n",
       "\n",
       "       [[0.10667396, 0.14034708, 0.13128717, ..., 0.12231664,\n",
       "         0.12716496, 0.12246663]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "import numpy as np\r\n",
    "predictions = np.argmax(test_pred_array, axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "train_tags[29]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "classes = np.argmax(predictions, axis = 1)\r\n",
    "print(classes)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}