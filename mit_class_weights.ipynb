{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit"
  },
  "interpreter": {
   "hash": "3898892d7e34557bb8499aff9aa0ccd3bf7bab375649613f01d0952879e4c360"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def get_dict_map(data, token_or_tag):\n",
    "    tok2idx = {}\n",
    "    idx2tok = {}\n",
    "    \n",
    "    if token_or_tag == 'token':\n",
    "        vocab = list(set(data['Wort'].to_list()))\n",
    "    else:\n",
    "        vocab = list(set(data['Attribut'].to_list()))\n",
    "    \n",
    "    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n",
    "    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}\n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#from keras.utils import to_categorical\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "def get_pad_train_test_val(data_group, data):\n",
    "    n_token = len(list(set(data['Wort'].to_list())))\n",
    "    n_tag = len(list(set(data['Attribut'].to_list())))\n",
    "\n",
    "    tokens = data_group['Word_idx'].to_list()\n",
    "    maxlen = max([len(s) for s in tokens])\n",
    "    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value=n_token-1)\n",
    "\n",
    "    tags = data_group['Tag_idx'].to_list()\n",
    "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value=tag2idx[\"O\"])\n",
    "\n",
    "    n_tags = len(tag2idx)\n",
    "    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n",
    "\n",
    "    train_tokens, test_tokens, train_tags, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)\n",
    "\n",
    "    print(\n",
    "        'train_tokens length:', len(train_tokens),\n",
    "        '\\ntest_tokens length: ', len(test_tokens),\n",
    "        '\\ntrain_tags length: ', len(train_tags),\n",
    "        '\\ntest_tags length: ', len(test_tags)\n",
    "    )\n",
    "\n",
    "    return train_tokens, test_tokens, train_tags, test_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## class weights berechnen\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "def generate_class_weights(class_series, multi_class=True, one_hot_encoded=False):\n",
    "  \"\"\"\n",
    "  Method to generate class weights given a set of multi-class or multi-label labels, both one-hot-encoded or not.\n",
    "  Some examples of different formats of class_series and their outputs are:\n",
    "    - generate_class_weights(['mango', 'lemon', 'banana', 'mango'], multi_class=True, one_hot_encoded=False)\n",
    "    {'banana': 1.3333333333333333, 'lemon': 1.3333333333333333, 'mango': 0.6666666666666666}\n",
    "    - generate_class_weights([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]], multi_class=True, one_hot_encoded=True)\n",
    "    {0: 0.6666666666666666, 1: 1.3333333333333333, 2: 1.3333333333333333}\n",
    "    - generate_class_weights([['mango', 'lemon'], ['mango'], ['lemon', 'banana'], ['lemon']], multi_class=False, one_hot_encoded=False)\n",
    "    {'banana': 1.3333333333333333, 'lemon': 0.4444444444444444, 'mango': 0.6666666666666666}\n",
    "    - generate_class_weights([[0, 1, 1], [0, 0, 1], [1, 1, 0], [0, 1, 0]], multi_class=False, one_hot_encoded=True)\n",
    "    {0: 1.3333333333333333, 1: 0.4444444444444444, 2: 0.6666666666666666}\n",
    "  The output is a dictionary in the format { class_label: class_weight }. In case the input is one hot encoded, the class_label would be index\n",
    "  of appareance of the label when the dataset was processed. \n",
    "  In multi_class this is np.unique(class_series) and in multi-label np.unique(np.concatenate(class_series)).\n",
    "  Author: Angel Igareta (angel@igareta.com)\n",
    "  \"\"\"\n",
    "  if multi_class:\n",
    "    # If class is one hot encoded, transform to categorical labels to use compute_class_weight   \n",
    "    if one_hot_encoded:\n",
    "      class_series = np.argmax(class_series, axis=1)\n",
    "  \n",
    "    # Compute class weights with sklearn method\n",
    "    class_labels = np.unique(class_series)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=class_series)\n",
    "    return dict(zip(class_labels, class_weights))\n",
    "  else:\n",
    "    # It is neccessary that the multi-label values are one-hot encoded\n",
    "    mlb = None\n",
    "    if not one_hot_encoded:\n",
    "      mlb = MultiLabelBinarizer()\n",
    "      class_series = mlb.fit_transform(class_series)\n",
    "\n",
    "    n_samples = len(class_series)\n",
    "    n_classes = len(class_series[0])\n",
    "\n",
    "    # Count each class frequency\n",
    "    class_count = [0] * n_classes\n",
    "    for classes in class_series:\n",
    "        for index in range(n_classes):\n",
    "            if classes[index] != 0:\n",
    "                class_count[index] += 1\n",
    "    \n",
    "    # Compute class weights using balanced method\n",
    "    class_weights = [n_samples / (n_classes * freq) if freq > 0 else 1 for freq in class_count]\n",
    "    class_labels = range(len(class_weights)) if mlb is None else mlb.classes_\n",
    "    return dict(zip(class_labels, class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "         satzId           Wort       Attribut  Word_idx  Tag_idx\n",
      "0             0          Broan        B-Brand     27322        2\n",
      "1             0  TEN136WWBroan              O     21752        4\n",
      "2             0       TEN136WW  B-Modelnumber     82916        6\n",
      "3             0       Overview              O     54579        4\n",
      "4             0            The              O     34588        4\n",
      "...         ...            ...            ...       ...      ...\n",
      "4928479   55339            AND              O     65913        4\n",
      "4928480   55339    MAINTENANCE              O     27239        4\n",
      "4928481   55339       Lockable              O     18760        4\n",
      "4928482   55339        control              O     34174        4\n",
      "4928483   55339          panel              O     76106        4\n",
      "\n",
      "[4928484 rows x 5 columns]\n",
      "<ipython-input-21-68cf3901bb79>:14: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  df_group = df.groupby(by = ['satzId'], as_index=False)['Wort', 'Attribut', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))\n",
      "       satzId                                               Wort  \\\n",
      "0           0  [Broan, TEN136WWBroan, TEN136WW, Overview, The...   \n",
      "1           2  [Imperial, IS1936PS1TWSB8SSImperial, IS1936PS1...   \n",
      "2           3  [Hestan, KRT485GDNGORHestan, KRT485GDNGOR, Ove...   \n",
      "3           5  [Hestan, KVP54PPHestan, KVP54PP, Overview, The...   \n",
      "4           6  [LG, LWS3063BDLG, LWS3063BD, Overview, This, 3...   \n",
      "...       ...                                                ...   \n",
      "39006   55334  [Wolf, ®, 36, \", Pro, Style, Gas, Range, -, St...   \n",
      "39007   55336  [LG, 24, \", Smooth, Black, Built, In, Dishwash...   \n",
      "39008   55337  [Whirlpool, ®, 4.3, Cu, ., Ft, ., Chrome, Shad...   \n",
      "39009   55338  [Thermador, ®, Masterpiece, ®, 36, \", Electric...   \n",
      "39010   55339  [ASKO, Classic, 4.1, Cu, ., Ft, ., White, Fron...   \n",
      "\n",
      "                                                Attribut  \\\n",
      "0      [B-Brand, O, B-Modelnumber, O, O, O, O, O, O, ...   \n",
      "1      [B-Brand, O, B-Modelnumber, O, O, O, O, O, O, ...   \n",
      "2      [B-Brand, O, B-Modelnumber, O, O, O, O, O, O, ...   \n",
      "3      [B-Brand, O, B-Modelnumber, O, O, O, O, O, O, ...   \n",
      "4      [B-Brand, O, B-Modelnumber, O, O, O, O, O, O, ...   \n",
      "...                                                  ...   \n",
      "39006  [B-Brand, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
      "39007  [B-Brand, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
      "39008  [B-Brand, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
      "39009  [B-Brand, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
      "39010  [B-Brand, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
      "\n",
      "                                                Word_idx  \\\n",
      "0      [27322, 21752, 82916, 54579, 34588, 15895, 192...   \n",
      "1      [80714, 92889, 51829, 54579, 97741, 8367, 9423...   \n",
      "2      [37464, 45116, 45224, 54579, 34588, 47907, 253...   \n",
      "3      [37464, 96174, 27918, 54579, 34588, 55210, 253...   \n",
      "4      [97189, 32893, 95407, 54579, 97741, 74686, 253...   \n",
      "...                                                  ...   \n",
      "39006  [82242, 79766, 40821, 25323, 39358, 61394, 237...   \n",
      "39007  [97189, 51250, 25323, 90221, 74228, 102637, 10...   \n",
      "39008  [50312, 79766, 83237, 21839, 25245, 19019, 252...   \n",
      "39009  [58494, 79766, 28117, 79766, 40821, 25323, 558...   \n",
      "39010  [74770, 39176, 52848, 21839, 25245, 19019, 252...   \n",
      "\n",
      "                                                 Tag_idx  \n",
      "0      [2, 4, 6, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, ...  \n",
      "1      [2, 4, 6, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, ...  \n",
      "2      [2, 4, 6, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, ...  \n",
      "3      [2, 4, 6, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, ...  \n",
      "4      [2, 4, 6, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, ...  \n",
      "...                                                  ...  \n",
      "39006  [2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
      "39007  [2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
      "39008  [2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
      "39009  [2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
      "39010  [2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
      "\n",
      "[39011 rows x 5 columns]\n",
      "train_tokens length: 35109 \n",
      "test_tokens length:  3902 \n",
      "train_tags length:  35109 \n",
      "test_tags length:  3902\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('Daten/trainingData300.csv',escapechar=\"\\\\\",sep=\",\",error_bad_lines=False,warn_bad_lines=False)\n",
    "#print(len(df))\n",
    "\n",
    "token2idx, idx2token = get_dict_map(df, 'token')\n",
    "tag2idx, idx2tag = get_dict_map(df, 'tag')\n",
    "\n",
    "df['Word_idx'] = df['Wort'].map(token2idx)\n",
    "df['Tag_idx'] = df['Attribut'].map(tag2idx)\n",
    "print(df)\n",
    "\n",
    "df_group = df.groupby(by = ['satzId'], as_index=False)['Wort', 'Attribut', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))\n",
    "print(df_group)\n",
    "train_tokens, test_tokens, train_tags, test_tags = get_pad_train_test_val(df_group, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{1: 42.857142857142854, 2: 42.857142857142854, 3: 42.857142857142854, 4: 0.1457725947521866, 5: 42.857142857142854, 6: 42.857142857142854, 7: 42.857142857142854}\n"
     ]
    }
   ],
   "source": [
    "# von den traing_tags die class weigths berechnen\n",
    "\n",
    "class_weights = generate_class_weights(train_tags[0], multi_class=True, one_hot_encoded=True)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_dim:  103349 \noutput_dim:  32 \ninput_length:  300 \nn_tags:  8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_dim = len(list(set(df['Wort'].to_list())))+1\n",
    "output_dim = 32\n",
    "input_length = max([len(s) for s in df_group['Word_idx'].tolist()])\n",
    "n_tags = len(tag2idx)\n",
    "print('input_dim: ', input_dim, '\\noutput_dim: ', output_dim, '\\ninput_length: ', input_length, '\\nn_tags: ', n_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "import tensorflow\n",
    "seed(1)\n",
    "tensorflow.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import tensorflow\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "\n",
    "def get_bilstm_lstm_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode='concat'))\n",
    "\n",
    "    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "    model.add(TimeDistributed(Dense(n_tags, activation=\"softmax\")))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'], sample_weight_mode='temporal')\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: Array korreckt befüllen:\n",
    "# https://github.com/keras-team/keras/issues/3653#issuecomment-761085597\n",
    "def generate_sample_weights(train_tags, class_weights): \n",
    "    #replaces values for up to 3 classes with the values from class_weights#\n",
    "    # ToDo: Train Tags sind OnHot Encodings \n",
    "    sample_weights = [np.where(y==0,class_weights[0],\n",
    "                        np.where(y==1,class_weights[1],\n",
    "                        np.where(y==2,class_weights[2],\n",
    "                        np.where(y==3,class_weights[3],\n",
    "                        np.where(y==4,class_weights[4],\n",
    "                        np.where(y==3,class_weights[5],\n",
    "                        np.where(y==4,class_weights[6],\n",
    "                        np.where(y==5,class_weights[7])))))))) for y in training_data]\n",
    "    return np.asarray(sample_weights)\n",
    "\n",
    "class_weights = {0: 0.1, 1:300.0, 2:300.0, 3:300.0, 4: 300.0, 5:300.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = np.zeros((39010, 300))\n",
    "# ToDo: Array korreckt befüllen:\n",
    "# https://github.com/keras-team/keras/issues/3653#issuecomment-761085597\n",
    "class_weights[:, 0] += 0.1\n",
    "class_weights[:, 1] += 300.0\n",
    "class_weights[:, 2] += 300.0\n",
    "class_weights[:, 3] += 300.0\n",
    "class_weights[:, 4] += 300.0\n",
    "class_weights[:, 5] += 300.0\n",
    "class_weights[:, 6] += 300.0\n",
    "class_weights[:, 7] += 300.0\n",
    "\n",
    "def train_model(X, y, model):\n",
    "    loss  = list()\n",
    "    for _ in range(1):\n",
    "        hist = model.fit(X, y, batch_size=256, verbose=1, epochs=1, validation_split=0.2, sample_weight=class_weights)\n",
    "        loss.append(hist.history['loss'][0])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 300, 32)           3307168   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 300, 64)           16640     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 300, 32)           12416     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 300, 8)            264       \n",
      "=================================================================\n",
      "Total params: 3,336,488\n",
      "Trainable params: 3,336,488\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bilstm_lstm = get_bilstm_lstm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "110/110 [==============================] - 852s 8s/step - loss: 5.5693 - accuracy: 0.9754 - val_loss: 3.5370 - val_accuracy: 0.9896\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "results['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/train300wWeights\\assets\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# Save the model\n",
    "filepath = './models/train300wWeights'\n",
    "# save_model(md, filepath)\n",
    "model_bilstm_lstm.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}